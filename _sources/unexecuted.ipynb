{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Unexecuted] Parameter estimation tutorial\n",
    "\n",
    "There are a quite a few steps to analyse GW data. Today we'll focus on the middle panel:\n",
    "![Screen-Shot-2022-11-21-at-10-35-12-pm-1.png](https://i.postimg.cc/dtLCYm7Y/Screen-Shot-2022-11-21-at-10-35-12-pm-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T23:35:34.166880Z",
     "start_time": "2023-02-14T23:35:29.483909Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install bilby[gw] --upgrade -q\n",
    "# NOTE: you'll have to restart your runtime after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T22:29:47.291839Z",
     "start_time": "2023-02-14T22:29:47.108241Z"
    },
    "lines_to_next_cell": 2,
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import bilby\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from corner.core import quantile\n",
    "from corner import corner\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "INJECTION_URL = \"https://sandbox.zenodo.org/record/1163982/files/injection_result.json?download=1\"\n",
    "GW150914_URL = \"https://sandbox.zenodo.org/record/1164558/files/GW150914_result.json?download=1\"\n",
    "GW200308_URL = \"https://zenodo.org/record/5546663/files/IGWN-GWTC3p0-v1-GW200308_173609_PEDataRelease_mixed_cosmo.h5?download=1\"\n",
    "\n",
    "\n",
    "\n",
    "def notebook_setup():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    logger = logging.getLogger(\"root\")\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    logger = logging.getLogger(\"bilby\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.rcParams[\"savefig.dpi\"] = 100\n",
    "    plt.rcParams[\"figure.dpi\"] = 100\n",
    "    plt.rcParams[\"font.size\"] = 16\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"Liberation Sans\"]\n",
    "    plt.rcParams[\"font.cursive\"] = [\"Liberation Sans\"]\n",
    "    plt.rcParams[\"mathtext.fontset\"] = \"custom\"\n",
    "    plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "\n",
    "def download(url: str, fname: str):\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    with open(fname, 'wb') as file, tqdm(\n",
    "        desc=f\"Downloading {fname}\",\n",
    "        total=total,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in resp.iter_content(chunk_size=1024):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "            \n",
    "\n",
    "\n",
    "notebook_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:04.586045Z",
     "start_time": "2023-02-14T14:40:04.582469Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "RE_RUN_SLOW_CELLS = False\n",
    "OUTDIR = \"outdir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference intro\n",
    "\n",
    "\n",
    "To start, lets take a look at Bayes' theorem:\n",
    "\n",
    "\\begin{align}\n",
    "\\rm{Posterior} &= \\frac{\\rm{Prior} * \\rm{Likelihood}}{\\rm{Evidence}}  \\\\\n",
    "\\implies P(\\theta|\\rm{data}) &= \\frac{\\pi(\\theta)\\  \\mathcal{L}(\\rm{data}|\\theta)}{\\int_{\\theta} \\pi(\\theta)\\ \\mathcal{L}(\\rm{data}|\\theta)\\ d\\theta}\\\\\n",
    " &= \\frac{\\pi(\\theta)\\  \\mathcal{L}(\\rm{data}|\\theta)}{\\mathcal{Z}(data)}\n",
    "\\end{align}\n",
    "\n",
    "For a primer on Bayesian inference in GW, please look at [Eric Thrane + Colm Talbot's paper](https://arxiv.org/abs/1809.02293).\n",
    "\n",
    "\n",
    "\n",
    "In this workshop we will focus more on how we can perform inference and not focus too much on the maths. \n",
    "\n",
    "Lets look at an example of how we can use Bayesian inference to analyse the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:04.603064Z",
     "start_time": "2023-02-14T14:40:04.591404Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "observation = np.array([\n",
    "    -1.65, 1.93, 2.88, -2.28, -1.02, 0.35, 1.49, 0.65, -1.95, \n",
    "    3.64, -2.47, 3.91, -2.16, 1.03, 0.6, 6.96, 1.07, -2.69, \n",
    "    -7.18, -0.94, -1.37, -1.09, -2.07, 6.28, 1.73, 1.0, 4.11,\n",
    "    1.29, -1.21, -1.06, 3.67, 0.91, 0.64, -0.4, 9.2, -3.51, \n",
    "    1.89, -2.49, 5.43, 2.36, 0.18, 0.01, 6.85, 2.25, 3.55, \n",
    "    3.43, 3.1, 3.98, -1.06, 6.79, 3.27, -1.62, -4.16, -0.19,\n",
    "    1.75, 6.18, -0.72, -1.4, 0.55, 4.85, 6.83, 10.35, 3.83, \n",
    "    5.46, -0.81, 0.91, 3.36, 2.01, 4.37, 4.03, 6.05, 2.62, \n",
    "    5.16, 3.57, 3.74, 7.07, 3.89, 9.91, 3.89, 3.41, 7.71, 2.79,\n",
    "    3.98, 4.91, 1.87, 2.65, 3.4, 3.42, 11.26, 5.06, 8.83, 2.87, \n",
    "    8.66, 3.95, 6.05, 8.2, 3.07, 2.88, 4.6, 3.84\n",
    "])\n",
    "time = np.array([\n",
    "    0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1,\n",
    "    1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, \n",
    "    2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, \n",
    "    3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, \n",
    "    4.8, 4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, \n",
    "    6.0, 6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 7.0, 7.1, \n",
    "    7.2, 7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9, 8.0, 8.1, 8.2, 8.3, \n",
    "    8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9.0, 9.1, 9.2, 9.3, 9.4, 9.5, \n",
    "    9.6, 9.7, 9.8, 9.9\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:04.826122Z",
     "start_time": "2023-02-14T14:40:04.606043Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.plot(time, observation, \"o\", label=\"raw data\", color='k', zorder=-10)\n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.set_ylabel(\"y\");\n",
    "    ax.set_xlim(min(time), max(time))\n",
    "\n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume:\n",
    "1) The observed data `d(t)` consists of the following: `d(t) = n(t) + s(t)`.\n",
    "2) The noise `n(t)` is `Gaussian white noise` (drawn from a Normal distribution).\n",
    "3) The signal `s(t)` is a straight-line signal. \n",
    "\n",
    "With these assumptions, we can start implementing our Bayesian inference pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T04:45:38.581262Z",
     "start_time": "2023-02-14T04:45:38.576423Z"
    }
   },
   "source": [
    "## Signal Model\n",
    "Assuming a straight line signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:04.832358Z",
     "start_time": "2023-02-14T14:40:04.828737Z"
    }
   },
   "outputs": [],
   "source": [
    "def signal_model(time, m, c):\n",
    "    return time * m + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priors\n",
    "Now we can write down some priors on the parameters `m` and `c` of this model. \n",
    "This is what we think our parameters can potentially be.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:04.839067Z",
     "start_time": "2023-02-14T14:40:04.834894Z"
    }
   },
   "outputs": [],
   "source": [
    "import bilby\n",
    "\n",
    "priors = bilby.core.prior.PriorDict(dict(\n",
    "    m=bilby.core.prior.TruncatedNormal(mu=0, sigma=1, minimum=0, maximum=3, name=\"m\"),\n",
    "    c=bilby.core.prior.Uniform(-5, 5, name=\"c\"),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T04:26:11.548881Z",
     "start_time": "2023-02-14T04:26:11.542184Z"
    }
   },
   "source": [
    "To test our prior, lets draw some prior samples and print some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:04.850696Z",
     "start_time": "2023-02-14T14:40:04.841313Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(priors.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot some histograms of these samples in a `corner` plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:05.406978Z",
     "start_time": "2023-02-14T14:40:04.852889Z"
    }
   },
   "outputs": [],
   "source": [
    "from corner import corner\n",
    "fig = corner(\n",
    "    pd.DataFrame(priors.sample(10000)),\n",
    "    plot_datapoints=False, \n",
    "    plot_contours=False, plot_density=True,\n",
    "    color=\"tab:gray\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here each column/row represents one parameter. \n",
    "\n",
    "The plots along the upper diagonal show the 1D histograms of the parameters.\n",
    "In the above, these represent `pi(m)` and `pi(c)` -- the prior distributions for `m` and `c`. \n",
    "\n",
    "The plots on the inside of the corner show the 2D histograms for the intersecting parameters.\n",
    "Here, the 2D join distribution is `pi(m,c)`.\n",
    "\n",
    "\n",
    "At this point, lets test out our model and priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:05.753725Z",
     "start_time": "2023-02-14T14:40:05.413084Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_model_on_data(samples, plot_each_sample=False, color=\"tab:blue\", ax=None):\n",
    "    m, c = samples['m'], samples['c']\n",
    "    \n",
    "    ys = np.array([signal_model(time, mi, ci) for mi, ci in zip(m, c)]).T\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    y_low, y_mean, y_up = np.quantile(ys, [0.05, 0.5, 0.95], axis=1)\n",
    "    ax.plot(time, y_mean, color=color, label=\"mean model\")\n",
    "    ax.fill_between(time, y_low, y_up, alpha=0.1, color=color, label=\"model 90%\")\n",
    "\n",
    "    if plot_each_sample:\n",
    "        for y in ys.T:\n",
    "            ax.plot(time, y, color=color, alpha=0.1)\n",
    "        \n",
    "    plot_data(ax)\n",
    "    ax.legend(frameon=True)\n",
    "    return fig\n",
    "\n",
    "\n",
    "    \n",
    "fig = plot_model_on_data(priors.sample(30), True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model can potentially fit the data! Lets try to use the Bayesian inference framework to help us get estimates on the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T04:48:03.692300Z",
     "start_time": "2023-02-14T04:48:03.678095Z"
    }
   },
   "source": [
    "## Likelihood\n",
    "\n",
    "Recall the assumption that `d(t) = n(t) + s(t)`, and that the noise is normally distributed. Using this, we can write:\n",
    "\n",
    "\\begin{align}\n",
    "n(t) &= s(t) - d(t)\\\\\n",
    "&= \\mathcal{N}(\\sigma=3, \\mu=0)\n",
    "\\end{align}\n",
    "\n",
    "[NOTE: we've hardcoded `sigma` here, but you could add in a prior on `sigma` as well]\n",
    "\n",
    "Hence, we can write out the signal-model likliood `L(data|m,c)` as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\rm{data}|m,c) = \\prod_j \\rm{Normal}_{\\rm{PDF}}(\\left[d - s_j\\right]; \\mu=0,\\sigma=3)\n",
    "\\end{align}\n",
    "\n",
    "This type of `Gaussian` likelihoods has already been coded up in `bilby`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:05.759930Z",
     "start_time": "2023-02-14T14:40:05.756482Z"
    }
   },
   "outputs": [],
   "source": [
    "likelihood = bilby.likelihood.GaussianLikelihood(time, observation, signal_model, sigma=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if we can compute a likelihood and dont get a nan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:05.766861Z",
     "start_time": "2023-02-14T14:40:05.762063Z"
    }
   },
   "outputs": [],
   "source": [
    "# we need to provide the parameter to compute the likeliood for\n",
    "likelihood.parameters = dict(m=0, c=0)\n",
    "print(f\"Log Likelihood (data| {likelihood.parameters}) = {likelihood.log_likelihood():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T06:05:55.422733Z",
     "start_time": "2023-02-14T06:05:55.420032Z"
    }
   },
   "source": [
    "## Brute force posterior computation\n",
    "\n",
    "We now have everything we need to compute our posterior! \n",
    "In the first pass, lets compute the posterior over a grid of `m` and `c` values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:05.984675Z",
     "start_time": "2023-02-14T14:40:05.769552Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_grid_of_m_c(n_per_dim, prior):\n",
    "    assert \"m\" in prior and \"c\" in prior\n",
    "    samples = pd.DataFrame(prior.sample(10000))\n",
    "    m_vals = np.linspace(min(samples.m), max(samples.m), n_per_dim)\n",
    "    c_vals = np.linspace(min(samples.c), max(samples.c), n_per_dim)\n",
    "    m_grid, c_grid = np.meshgrid(m_vals, c_vals)\n",
    "    return pd.DataFrame(dict(m=m_grid.flatten(), c=c_grid.flatten()))\n",
    "\n",
    "\n",
    "def plot_grid(m_c_grid, c=\"tab:blue\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(3,3))\n",
    "    ax.scatter(m_c_grid.m, m_c_grid.c, c=c, s=12.5, marker='s',alpha=0.5)\n",
    "    ax.set_xlabel(\"m\");\n",
    "    ax.set_ylabel(\"c\");\n",
    "    return ax.figure\n",
    "    \n",
    "\n",
    "m_c_grid = get_grid_of_m_c(100, priors)\n",
    "fig = plot_grid(m_c_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the prior, likelhood and posterior at each grid-point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:08.265801Z",
     "start_time": "2023-02-14T14:40:05.987267Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_brute_force_analysis(likelihood, prior, samples_list):\n",
    "    n = len(samples_list)\n",
    "    log_likelihoods = np.zeros(n)\n",
    "    log_priors = np.zeros(n)\n",
    "    log_posteriors = np.zeros(n) \n",
    "    log_evidence = 0 \n",
    "    for i in tqdm(range(n), desc=\"Brute force LnL computation\"):\n",
    "        likelihood.parameters = samples_list[i]\n",
    "        log_likelihoods[i]=likelihood.log_likelihood()\n",
    "        log_priors[i] = prior.ln_prob(samples_list[i])\n",
    "        log_posteriors[i] = log_priors[i] + log_likelihoods[i]\n",
    "    log_evidence = np.logaddexp.reduce(log_posteriors+log_priors)\n",
    "    \n",
    "    return dict(\n",
    "        samples=pd.DataFrame(samples_list),\n",
    "        log_evidence=log_evidence,\n",
    "        likelihood=np.exp(log_likelihoods),\n",
    "        prior=np.exp(log_priors),\n",
    "        posterior=np.exp(log_posteriors)\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_grids_of_brute_result(res, xlims=None, log_vals=False):\n",
    "    fig, axes = plt.subplots(1,3, sharey=True, figsize=(7.5,3))\n",
    "    pi, post, like = res['prior'], res['posterior'], res['likelihood']\n",
    "    prefix = \"\"\n",
    "    if log_vals:\n",
    "        pi, post, like = np.log(pi), np.log(post), np.log(like)\n",
    "        prefix = \"log \"\n",
    "    axes[0].set_title(prefix+\"$\\pi(m,c)$\")\n",
    "    plot_grid(m_c_grid, c=pi, ax=axes[0])\n",
    "    axes[1].set_title(prefix+\"$L(data|m,c)$\")\n",
    "    plot_grid(m_c_grid, c=like, ax=axes[1])\n",
    "    axes[2].set_title(prefix+\"$p(m,c|data)$\")\n",
    "    plot_grid(m_c_grid, c=post, ax=axes[2])\n",
    "    if xlims is not None:\n",
    "        for ax in axes:\n",
    "            ax.set_xlim(*xlims)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "    \n",
    "brute_result = run_brute_force_analysis(likelihood, priors, m_c_grid.to_dict('records'))\n",
    "fig = plot_grids_of_brute_result(brute_result)\n",
    "print(f\"Brute force log evidence = {brute_result['log_evidence']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the answer is around \n",
    "\\begin{align}\n",
    "0<m<1, 1 < c < -2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even compute the marginal posteriors:\n",
    "\n",
    "\\begin{align}\n",
    "p(m|d)&= \\int_c p(m,c|d)\\ dc\\\\\n",
    "p(c|d)&= \\int_m p(m,c|d)\\ dm\n",
    "\\end{align}\n",
    "These will give us the estimated values for the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:08.678482Z",
     "start_time": "2023-02-14T14:40:08.268355Z"
    }
   },
   "outputs": [],
   "source": [
    "from corner.core import quantile\n",
    "\n",
    "def get_marginalised_posterior(parameter, res):\n",
    "    grid = res[\"samples\"].copy()\n",
    "    unique_values = np.unique(grid[parameter])\n",
    "    log_post = np.zeros(len(unique_values))\n",
    "    grid['log_post'] = np.log(res['posterior'])\n",
    "    for i, value in enumerate(unique_values):\n",
    "        subset = grid[grid[parameter] == value]\n",
    "        log_post[i] = np.logaddexp.reduce(subset[\"log_post\"])\n",
    "    return unique_values, np.exp(log_post)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(5,2))\n",
    "for ax, p in zip(axes, ['m', 'c']):\n",
    "    x, p_of_x = get_marginalised_posterior(p, brute_result)\n",
    "    ax.plot(x, p_of_x, color=\"tab:purple\")\n",
    "    low, mean, up = quantile(x, q=[0.05, 0.5, 0.95], weights=p_of_x)\n",
    "    low, up = mean-low, up-mean\n",
    "    title = r\"${{{0:.2f}}}_{{-{1:.2f}}}^{{+{2:.2f}}}$\".format(mean, low, up)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also plot the posterior predictive check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:08.921126Z",
     "start_time": "2023-02-14T14:40:08.680140Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_model_on_data(\n",
    "    m_c_grid.sample(30, weights=brute_result['posterior']),\n",
    "    color=\"tab:purple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T08:18:52.145350Z",
     "start_time": "2023-02-14T08:18:52.141328Z"
    }
   },
   "source": [
    "## Sampling from the posterior\n",
    "\n",
    "There are some drawbacks from brute-force posterior estimation. \n",
    "\n",
    "What happens when the number of parameters increases to 15?\n",
    "\n",
    "Drawing samples from the posterior and trying to estimate the likelihood may be a cheaper approach.\n",
    "\n",
    "Below is some code to demonstrate how we can use bilby + a `nested` sampler \"dynesty\" to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:21.384018Z",
     "start_time": "2023-02-14T14:40:08.924607Z"
    }
   },
   "outputs": [],
   "source": [
    "def sampler_run():\n",
    "    result = bilby.run_sampler(\n",
    "        likelihood=likelihood,\n",
    "        priors=priors,\n",
    "        sampler=\"dynesty\",\n",
    "        nlive=500,\n",
    "        sample=\"unif\",\n",
    "        outdir=OUTDIR,\n",
    "        label=\"linear_regression\",\n",
    "    )\n",
    "    return result\n",
    "\n",
    "sampler_result = sampler_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make some plots. \n",
    "\n",
    "First, lets plot the `corner` plot, and overplot the prior (in green) in the 1D distribution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:21.941585Z",
     "start_time": "2023-02-14T14:40:21.386426Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = sampler_result.plot_corner(priors=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check how well we have done (comparing against the true value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:22.210041Z",
     "start_time": "2023-02-14T14:40:21.944283Z"
    }
   },
   "outputs": [],
   "source": [
    "truths = {'c': 0.2, 'm': 0.5, 'sigma': 3}\n",
    "\n",
    "fig = plot_model_on_data(sampler_result.posterior.sample(1000))\n",
    "fig.axes[0].plot(time, signal_model(time, m=truths['m'], c=truths['c']), color=\"tab:orange\", ls='--', label=\"True\")\n",
    "fig.axes[0].legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets finally compare the brute-force results and the nested-sampling results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:23.167202Z",
     "start_time": "2023-02-14T14:40:22.212323Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def overplot_sampler_and_brute_force(sampler_result, brute_result, truths):\n",
    "    truths = dict(m=truths[\"m\"], c=truths[\"c\"])\n",
    "    fig = sampler_result.plot_corner(parameters=truths, save=False)\n",
    "    # overplot the grid estimates\n",
    "    axes = fig.axes\n",
    "    \n",
    "    # add marginals \n",
    "    for p, axi in zip([\"m\", \"c\"],[0, 3]):\n",
    "        ax = axes[axi].twinx()\n",
    "        ax.set_yticks([])\n",
    "        x, p_of_x = get_marginalised_posterior(p, brute_result)\n",
    "        ax.plot(x, p_of_x, color=\"tab:purple\", alpha=0.9)\n",
    "        ax.set_ylim(0, 1.15 * np.max(p_of_x))\n",
    "        \n",
    "    \n",
    "    samples_grid = brute_result['samples']\n",
    "    n_cells = int(np.sqrt(len(samples_grid)))\n",
    "    m_grid = samples_grid['m'].values.reshape(n_cells, n_cells)\n",
    "    c_grid = samples_grid['c'].values.reshape(n_cells, n_cells)\n",
    "    posterior_grid = brute_result['posterior'].reshape(n_cells, n_cells)\n",
    "    axes[2].contourf(\n",
    "        m_grid, c_grid, posterior_grid, \n",
    "        levels=np.quantile(posterior_grid, [0.95, 0.99, 1]),\n",
    "        cmap=\"Purples\"\n",
    "    )\n",
    "    axes[1].plot([],[], label=\"Sampler\", color=\"tab:blue\")\n",
    "    axes[1].plot([],[], label=\"Brute-Force\", color=\"tab:purple\")\n",
    "    axes[1].plot([],[], label=\"True\", color=\"tab:orange\")\n",
    "    axes[1].legend(frameon=False, loc='upper right')\n",
    "    print(f\"Brute force ln_evidence: {brute_result['log_evidence']}\")\n",
    "    print(f\"Sampler ln_evidence: {sampler_result.log_evidence}\")\n",
    "\n",
    "\n",
    "overplot_sampler_and_brute_force(sampler_result, brute_result, truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sets of results match up quite well! Even the log-evidences are comparable. \n",
    "\n",
    "Lets move on to GW signals! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBC GW  Signal Model\n",
    "\n",
    "The GW from compact binary coalescence (CBC) systems can be modeled using the following parameters:\n",
    "\n",
    "- 2 mass parameters (eg m1, m2)\n",
    "- 6 spin parameters (eg s1x, s1y, s1z, ...)\n",
    "- 2 tidal deformation parameters (for neutron stars, lambda1, lambda2)\n",
    "- 2 orbital eccentricity parameters (e, arg of periastron) \n",
    "- 7 extrinsic parameters (distance, sky-loc, timing, phase)\n",
    "\n",
    "\n",
    "Some of these are shown here:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://gcdnb.pbrd.co/images/EfwMgftuS4L5.png?o=1\" width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Additionally, there are quite a few re-parameterisations of these. \n",
    "\n",
    "For example, instead of the component masses $m_1, m_2$, one could also use the \n",
    "- `mass_ratio`: `q=m_2/m_1` where `m_1>m_2`, and \n",
    "- `chirp_mass`: \n",
    "\\begin{align}\n",
    "\\mathcal{M} = \\frac{(m_1 m_2)^{3/5}}{(m_1 + m_2)^{1/5}}\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we can use parameterize the dimensionless spins with the following 6 parameters:\n",
    "- `chi_i` or $a_i$ : the spin magnitude (`0<a_i<1`) \n",
    "- `theta_i` or `tilt_i`: the angle of the component binary wrt the `z` axis (along the orbital axis)\n",
    "- `phi_12`: the difference between the component spins projected on the `xy` plane (along the orbital plane)\n",
    "-  `theta_jl`: the angle between the orbital angular momentum `L` vector and the total angular momentum vector `J`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lets go over all the parameters in more detail:\n",
    "\n",
    "\n",
    "````{tab} Intrinsic parameters\n",
    "\n",
    "**INTRINSIC PARAMETERS**\n",
    "\n",
    "* Mass: 2D\n",
    "  - Usually uniform in two mass parameters\n",
    "  - Component masses widely used (`m_1, m_2`)\n",
    "  - \"Chirp\" mass and mass ratio a more convenient basis\n",
    "  - Usually specified in the \"detector-frame\", redshifted relative to true \"source-frame\" mass\n",
    "* Dimensionless Spin: 6D\n",
    "  - Fully precessing (see animation)\n",
    "    - usually uniform in magnitude and isotropic in orientation\n",
    "  - Only spin aligned with orbital angular momentum - set planar spin components to zero\n",
    "    - same prior on aligned spin as in the precessing case\n",
    "    - uniform in aligned spin\n",
    "  - Zero spin\n",
    "    - all spin components are zero\n",
    "    - smaller space to sample\n",
    "* Orbital eccentricity: 1D/2D \n",
    "  - Often ignored (eccentricity and argument of periastron)\n",
    "* Matter effects (for neutron stars): 2D\n",
    "  - Two tidal deformability parameters `Lambda_i`\n",
    "  - Parameters describing the neutron star equation of state (EoS)\n",
    "    - Variable number\n",
    "    - Zero-dimensional for fixed equation of state\n",
    "    \n",
    "````\n",
    "````{tab} Extrinsic parameters \n",
    "\n",
    "**EXTRINSIC PARAMETERS**\n",
    "\n",
    "* Location: 3D\n",
    "  - ra, dec, distance\n",
    "  - Usually isotropic over the sky\n",
    "  - Distance prior uniform in volume\n",
    "    - Should include cosmological effects\n",
    "  - Use host galaxy location, e.g., GW170817, S190521g(?)\n",
    "* Orientation: 4D\n",
    "  - Three Euler angles (phase, inclination, polarisation)\n",
    "    - Assumed to be distributed isotropically\n",
    "* Merger time\n",
    "    - Uniform based on expected uncertainty in trigger time\n",
    "    - Typically ~ 0.1s\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: increasing the spin in the x-y components can lead to precession.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://media4.giphy.com/media/NF76iRgEerkx0qrsD0/giphy.gif?cid=790b76113d85f36207d50f4d1f42feead29a3c4cb8f2f1a6&rid=giphy.gif&ct=g\" width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Such a highly precessing system may be found in globular clusters/AGNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lets plot our own waveform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:23.185960Z",
     "start_time": "2023-02-14T14:40:23.170474Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import bilby\n",
    "from bilby.gw.conversion import chirp_mass_and_mass_ratio_to_component_masses\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "bilby_logger = logging.getLogger(\"bilby\")\n",
    "bilby_logger.setLevel(logging.ERROR)\n",
    "\n",
    "def make_waveform_generator(approximant=\"IMRPhenomPv2\", duration=4, sampling_frequency=1024,\n",
    "                            frequency_domain_source_model=bilby.gw.source.lal_binary_black_hole, ):\n",
    "    \"\"\"Create a waveform generator\"\"\"\n",
    "    generator_args = dict(\n",
    "        duration=duration,\n",
    "        sampling_frequency=sampling_frequency,\n",
    "        frequency_domain_source_model=frequency_domain_source_model,\n",
    "        waveform_arguments=dict(\n",
    "            waveform_approximant=approximant,\n",
    "            reference_frequency=20.,\n",
    "        )\n",
    "    )\n",
    "    return bilby.gw.WaveformGenerator(**generator_args)\n",
    "\n",
    "\n",
    "def compute_waveform(waveform_generator, signal_parameters={}):\n",
    "    \"\"\"Compute the waveform\"\"\"\n",
    "    parameters = dict(\n",
    "        # two mass parameters\n",
    "        mass_1=30, mass_2=30,\n",
    "        # 6 spin parameters\n",
    "        a_1=0.0, a_2=0.0, tilt_1=0, tilt_2=0, phi_jl=0, phi_12=0,\n",
    "        # 2 tidal deformation parameters (for NS)\n",
    "        lambda_1=0, lambda_2=0,\n",
    "        # 7 extrinsic parameters (skyloc, timing, phase, etc)\n",
    "        luminosity_distance=1000,\n",
    "        theta_jn=0, ra=0, dec=0,\n",
    "        psi=0, phase=0,\n",
    "        geocent_time=0,\n",
    "    )\n",
    "    parameters.update(signal_parameters)\n",
    "    bilby_logger.info(f\"computing waveform with parameters: {parameters}\")\n",
    "    h = waveform_generator.time_domain_strain(parameters)\n",
    "    t = waveform_generator.time_array\n",
    "    approximant = waveform_generator.waveform_arguments[\"waveform_approximant\"]\n",
    "    delta_t = 1. / waveform_generator.sampling_frequency\n",
    "\n",
    "    if \"IMR\" in approximant:\n",
    "        # IMR templates the zero of time is at max amplitude (merger)\n",
    "        # thus we roll the waveform back a bit\n",
    "        for pol in h.keys():\n",
    "            h[pol] = np.roll(h[pol], - len(h[pol]) // 3)\n",
    "\n",
    "    h_phase = np.unwrap(np.arctan2(h['cross'], h['plus']))\n",
    "    h_freq = np.diff(h_phase) / (2 * np.pi * delta_t)\n",
    "    # nan the freqs after the merger\n",
    "    h_freq[np.argmax(h_freq):] = np.nan\n",
    "    return h, h_freq, t\n",
    "\n",
    "\n",
    "def plot_waveform(waveform_generator, signal_parameters={}, fig=None, polarisation='plus', label=\"\", color=\"tab:blue\"):\n",
    "    h, h_freq, h_time = compute_waveform(waveform_generator, signal_parameters)\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "    else:\n",
    "        ax = fig.axes[0]\n",
    "    if polarisation =='both':\n",
    "        ax.plot(h_time, h['plus'], alpha=0.4, label='plus')\n",
    "        ax.plot(h_time, h['cross'], alpha=0.4, label='cross', ls=\"--\")\n",
    "        ax.legend(frameon=False)\n",
    "    else:\n",
    "        ax.plot(h_time, h[polarisation], alpha=0.5, label=label, color=color)\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"Strain\")\n",
    "    ax.set_xlim(min(h_time), max(h_time))\n",
    "    # remove whitespace between subplots\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:23.518774Z",
     "start_time": "2023-02-14T14:40:23.188852Z"
    }
   },
   "outputs": [],
   "source": [
    "waveform_generator = make_waveform_generator(approximant=\"IMRPhenomPv2\")\n",
    "bilby_logger.setLevel(logging.DEBUG)\n",
    "fig = plot_waveform(waveform_generator, dict(mass_1=20., mass_2=25., psi=0.5), polarisation='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What happens if we increase chirp-mass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:24.040896Z",
     "start_time": "2023-02-14T14:40:23.521004Z"
    }
   },
   "outputs": [],
   "source": [
    "bilby_logger.setLevel(logging.ERROR) # turn off logging\n",
    "for mc in np.linspace(20, 60, 3):\n",
    "    m1, m2 = chirp_mass_and_mass_ratio_to_component_masses(mc, 1)\n",
    "    signal_parameters = dict(mass_1=m1, mass_2=m2)\n",
    "    fig = plot_waveform(waveform_generator, signal_parameters, label=r\"$\\mathcal{M} = \"+ f\"{int(mc)}\" + \"M_{\\odot}$\")\n",
    "    fig.axes[0].axis('off') \n",
    "    fig.axes[0].legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T12:17:21.221268Z",
     "start_time": "2023-02-14T12:17:21.214509Z"
    }
   },
   "source": [
    "Playing around with some interactive plots can also help build some intuition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:24.290454Z",
     "start_time": "2023-02-14T14:40:24.043593Z"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "@interact(a_1=(0.01, 1.0, 0.1))\n",
    "def interact_a1(a_1=0.5):\n",
    "    fig = plot_waveform(waveform_generator, dict(a_1=0.01, a_2=1), label=f\"a_1=0.01\", color=\"tab:red\")\n",
    "    fig = plot_waveform(waveform_generator, dict(a_1=a_1, a_2=1), fig=fig, label=f\"a_1={a_1:.2f}\", color=\"tab:blue\")\n",
    "    fig.axes[0].axis('off') \n",
    "    fig.axes[0].legend(frameon=False)\n",
    "    fig.axes[0].set_xlim(1.2, 2.8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:24.589383Z",
     "start_time": "2023-02-14T14:40:24.293980Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@interact(dist=(1000, 10000, 500))\n",
    "def interact_dist(dist=2000):\n",
    "    fig = plot_waveform(waveform_generator, dict(luminosity_distance=1000), label=f\"d=1000 Mpc\", color=\"tab:red\")\n",
    "    fig = plot_waveform(waveform_generator, dict(luminosity_distance=dist), fig=fig, label=f\"d={dist} Mpc\", color=\"tab:blue\")\n",
    "    fig.axes[0].legend(frameon=False)\n",
    "    fig.axes[0].set_ylim(-7e-22,7e-22)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBC Parameter Estimation\n",
    "\n",
    "Finally, we get to actual parameter estimation for gravitational waves! \n",
    "\n",
    "## Simulate a signal \n",
    "\n",
    "We start by first simulating a signal (using the same code as the previous section for waveform generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:24.600417Z",
     "start_time": "2023-02-14T14:40:24.591925Z"
    }
   },
   "outputs": [],
   "source": [
    "import bilby\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "bilby_logger = logging.getLogger(\"bilby\")\n",
    "bilby_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Simulate signal\n",
    "duration, sampling_freq, min_freq = 4, 1024., 20\n",
    "injection_parameters = dict(\n",
    "    mass_1=36.0, mass_2=29.0,  # 2 mass parameters\n",
    "    a_1=0.1, a_2=0.1, tilt_1=0.0, tilt_2=0.0, phi_12=0.0, phi_jl=0.0,  # 6 spin parameters\n",
    "    ra=1.375, dec=-1.2108, luminosity_distance=2000.0, theta_jn=0.0,  # 7 extrinsic parameters\n",
    "    psi=2.659, phase=1.3,\n",
    "    geocent_time=1126259642.413,\n",
    ")\n",
    "inj_m1, inj_m2 = injection_parameters['mass_1'], injection_parameters['mass_2']\n",
    "inj_chirp_mass = bilby.gw.conversion.component_masses_to_chirp_mass(inj_m1, inj_m2)\n",
    "inj_q = bilby.gw.conversion.component_masses_to_mass_ratio(inj_m1, inj_m2)\n",
    "\n",
    "waveform_generator = bilby.gw.WaveformGenerator(\n",
    "    duration=duration,\n",
    "    sampling_frequency=sampling_freq,\n",
    "    frequency_domain_source_model=bilby.gw.source.lal_binary_black_hole,\n",
    "    parameter_conversion=bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters,\n",
    "    waveform_arguments=dict(\n",
    "        waveform_approximant=\"IMRPhenomD\",\n",
    "        reference_frequency=20.0,\n",
    "        minimum_frequency=min_freq,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:24.842786Z",
     "start_time": "2023-02-14T14:40:24.603274Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_waveform(waveform_generator, injection_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject the signal into an interferometer's data stream\n",
    "\n",
    "We can now simulate some detector noise and inject this signal into the detector-noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:25.572455Z",
     "start_time": "2023-02-14T14:40:24.854705Z"
    },
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "# Inject the signal into 1 detectors LIGO-Hanford (H1) at design sensitivity\n",
    "ifos = bilby.gw.detector.InterferometerList([\"H1\"])\n",
    "ifos.set_strain_data_from_power_spectral_densities(\n",
    "    sampling_frequency=sampling_freq,\n",
    "    duration=duration,\n",
    "    start_time=injection_parameters[\"geocent_time\"] - 2,\n",
    ")\n",
    "ifos.inject_signal(\n",
    "    waveform_generator=waveform_generator, parameters=injection_parameters\n",
    ")\n",
    "\n",
    "for interferometer in ifos:\n",
    "    analysis_data = abs(interferometer.frequency_domain_strain)\n",
    "    fig = plt.figure()\n",
    "    plt.loglog(interferometer.frequency_array, analysis_data, label=\"Data\", color=\"tab:orange\", alpha=0.25)\n",
    "    plt.loglog(interferometer.frequency_array, abs(interferometer.amplitude_spectral_density_array),\n",
    "               label=\"ASD (estimated noise)\", color=\"tab:orange\")\n",
    "    plt.xlim(interferometer.minimum_frequency, interferometer.maximum_frequency)\n",
    "    plt.xlabel(\"Frequency [Hz]\")\n",
    "    plt.ylabel(r'Strain [strain/$\\sqrt{\\rm Hz}$]')\n",
    "    plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T13:11:35.080704Z",
     "start_time": "2023-02-14T13:11:35.072612Z"
    }
   },
   "source": [
    "### Notes about noise:\n",
    "\n",
    "- Lots of short duration glitches\n",
    "- The main contribution to LIGO/Virgo/KAGRA data is coloured Gaussian noise.\n",
    "- Constant frequency \"lines\"\n",
    "\n",
    "\n",
    "The Gaussian noise is described by the noise amplitude (power) spectral density, ASD (PSD).\n",
    "\n",
    "Most conveniently described in the frequency domain using a [circularly-symmetric complex normal distribution](https://en.wikipedia.org/wiki/Complex_normal_distribution#Circularly-symmetric_normal_distribution).\n",
    "\n",
    "\n",
    "#### Power/Ampitude-spectral-density generation (PSD/ASD)\n",
    "- Average over data (normally do this)\n",
    "  - Divide a long chunk of data and average the power in each chunk\n",
    "  - Assumes the PSD does not vary with time\n",
    "  - Assumes data are Gaussian over the full time stretch\n",
    "  - Has well defined statistical uncertainty\n",
    "- Fit a parameterised model to the data (eg using instrumental info from Dan Brown's talk)\n",
    "  - E.g., `BayesLine`\n",
    "  - Describing the PSD requires lots of parameters\n",
    "  - Estimating all these parameters is difficult and computationally expensive\n",
    "  - Especially expensive marginalise over the uncertainty in the model while fitting CBC signal models\n",
    "  \n",
    "#### Glitches\n",
    "- Short term, terrestrial, non-Gaussian transients\n",
    "- Can bias parameter estimation if overlapping signal\n",
    "- Can bias PSD estimation\n",
    "To mitigate:\n",
    "\n",
    "- Zero out data containing glitch, \"gating\"\n",
    "  - Also potentially removes signal\n",
    "  - Can bias PSD estimation if care is not taken\n",
    "- Fit a parameterised model to remove the glitch\n",
    "  - E.g., `BayesWave`\n",
    "  - Describing the PSD requires lots of parameters\n",
    "  - Estimating all these parameters is difficult and computationally expensive\n",
    "  - Especially expensive marginalise over the uncertainty in the model while fitting CBC signal models\n",
    "\n",
    "#### Lines\n",
    "\n",
    "- Data at specific frequencies are persistently non-Gaussian and non-stationary\n",
    "- Large amplitude can cause spectral leakage\n",
    "  - Need to apply a window\n",
    "- Many have known causes\n",
    "  - Suspension \"violin\" modes\n",
    "  - Calibration lines\n",
    "  - Mains power lines\n",
    "  - ...\n",
    "- Some can be subtracted (\"cleaned\") from the data stream\n",
    "- Can be correlated between detectors\n",
    "- Commonly not analysed when searching for long-duration signals\n",
    "- Included in compact binary coalescence parameter estimation (for now...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create priors for analysis\n",
    "\n",
    "Since sampling all parameters will take a long time, we set delta-functions to all but one mass parameter (chirp-mass). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:29.334308Z",
     "start_time": "2023-02-14T14:40:25.574733Z"
    },
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "# We sample in chirp-mass and mass-ratio, however--these are quite un-astrophysical priors\n",
    "# but in post-processing convert to uniform-in-component masses \n",
    "priors = bilby.gw.prior.BBHPriorDict()\n",
    "for key in [\n",
    "    \"a_1\",\n",
    "    \"a_2\",\n",
    "    \"tilt_1\",\n",
    "    \"tilt_2\",\n",
    "    \"phi_12\",\n",
    "    \"phi_jl\",\n",
    "    \"psi\",\n",
    "    \"ra\",\n",
    "    \"dec\",\n",
    "    \"geocent_time\",\n",
    "    \"phase\",\n",
    "    \"theta_jn\",\n",
    "    \"luminosity_distance\",\n",
    "]:\n",
    "    priors[key] = injection_parameters[key]\n",
    "priors[\"mass_ratio\"] = inj_q\n",
    "priors[\"chirp_mass\"] = bilby.gw.prior.UniformInComponentsChirpMass(\n",
    "    minimum=inj_chirp_mass - 5,\n",
    "    maximum=inj_chirp_mass + 5\n",
    ")\n",
    "\n",
    "# Perform a check that the prior does not extend to a parameter space longer than the data\n",
    "priors.validate_prior(duration, min_freq);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making some plots of the priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:29.489581Z",
     "start_time": "2023-02-14T14:40:29.335843Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mc = priors.sample(10000)['chirp_mass']\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "ax.hist(mc, histtype='step', bins=50)\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(\"Chirp Mass \" + r\"$M_{\\odot}$\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the likelihood to use for analysis\n",
    "\n",
    "Similar to the line-signal in Gaussian noise case, we assume that the GW signal is embedded in some Gaussian noise. We also assume that the noise is stationary.\n",
    "\n",
    "With these asumptions, we can use the [Whittle likelihood](https://en.wikipedia.org/wiki/Whittle_likelihood): \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(d | \\theta) = \\prod^{\\rm{H, L, V}}_{i}\\ \\prod^{20-2048Hz}_{j} \\frac{1}{2 \\pi\\  \\rm{PSD}_{i,j} } \\exp\\left( -\\frac{|\\tilde{d}_{i,j} - \\tilde{h}_{i,j}(\\theta)|^2}{2\\ \\rm{PSD}_{i,j} } \\right).\n",
    "\\end{align}\n",
    "\n",
    "Here:\n",
    "- `d` is the frequency-domain strain data, \n",
    "- `h` is the template waveform\n",
    "\n",
    "This can be instantiated in bilby like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:29.496876Z",
     "start_time": "2023-02-14T14:40:29.492756Z"
    }
   },
   "outputs": [],
   "source": [
    "# the IFOs contain the data and PSD for each detector\n",
    "# the waveform generator can generate differnt h(t)\n",
    "likelihood = bilby.gw.GravitationalWaveTransient(\n",
    "    interferometers=ifos, \n",
    "    waveform_generator=waveform_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T03:52:45.680092Z",
     "start_time": "2023-02-14T03:52:45.677592Z"
    }
   },
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:35.767548Z",
     "start_time": "2023-02-14T14:40:29.500578Z"
    }
   },
   "outputs": [],
   "source": [
    "if RE_RUN_SLOW_CELLS:\n",
    "    # Run sampler.  In this case we're going to use the `dynesty` sampler\n",
    "    result = bilby.run_sampler(\n",
    "        likelihood=likelihood,\n",
    "        priors=priors,\n",
    "        sampler=\"dynesty\",\n",
    "        npoints=250,\n",
    "        nlive=100, walks=25,\n",
    "        dlogz=0.1,\n",
    "        injection_parameters=injection_parameters,\n",
    "        outdir=OUTDIR,\n",
    "        label=\"injection\",\n",
    "        conversion_function=bilby.gw.conversion.generate_all_bbh_parameters,\n",
    "        result_class=bilby.gw.result.CBCResult,\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping sampling...\")\n",
    "    fn = f\"{OUTDIR}/injection_result.json\"\n",
    "    download(INJECTION_URL, fn)\n",
    "    result = bilby.gw.result.CBCResult.from_json(filename=fn)\n",
    "    print(\"Loaded result!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:43.654141Z",
     "start_time": "2023-02-14T14:40:35.769850Z"
    }
   },
   "outputs": [],
   "source": [
    "result.plot_corner(parameters=[\"mass_1\", \"mass_2\"], truths=[inj_m1, inj_m2], save=False)\n",
    "for interferometer in ifos:\n",
    "    fig = result.plot_interferometer_waveform_posterior(\n",
    "        interferometer=interferometer, save=False\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:43.661305Z",
     "start_time": "2023-02-14T14:40:43.656740Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets clear up some memory before proceeding to the next section!\n",
    "del result\n",
    "del ifos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of GW150914\n",
    "\n",
    "Lets analyse a real event! \n",
    "\n",
    "## Setup\n",
    "First we import some functions so we dont need to put in the full path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:40:44.158436Z",
     "start_time": "2023-02-14T14:40:43.663421Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bilby\n",
    "from bilby import run_sampler\n",
    "from bilby.core.prior import Constraint, Uniform\n",
    "from bilby.gw.conversion import (\n",
    "    convert_to_lal_binary_black_hole_parameters,\n",
    "    generate_all_bbh_parameters\n",
    ")\n",
    "from bilby.gw.detector.networks import InterferometerList\n",
    "from bilby.gw.detector.psd import PowerSpectralDensity\n",
    "from bilby.gw.likelihood import GravitationalWaveTransient\n",
    "from bilby.gw.prior import BBHPriorDict\n",
    "from bilby.gw.result import CBCResult\n",
    "from bilby.gw.source import lal_binary_black_hole\n",
    "from bilby.gw.utils import get_event_time\n",
    "from bilby.gw.waveform_generator import WaveformGenerator\n",
    "from gwpy.plot import Plot as GWpyPlot\n",
    "from gwpy.timeseries import TimeSeries\n",
    "import os\n",
    "import logging \n",
    "bilby_logger = logging.getLogger(\"bilby\")\n",
    "bilby_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading IFO data\n",
    "\n",
    "Now we download the raw data and make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T15:17:43.741326Z",
     "start_time": "2023-02-14T15:17:40.316699Z"
    }
   },
   "outputs": [],
   "source": [
    "interferometers = InterferometerList([\"H1\", \"L1\"])\n",
    "trigger_time = get_event_time(\"GW150914\")\n",
    "\n",
    "start_time = trigger_time - 3\n",
    "duration = 4\n",
    "end_time = start_time + duration\n",
    "roll_off = 0.2\n",
    "    \n",
    "\n",
    "\n",
    "# Get raw data\n",
    "raw_data = {}\n",
    "for ifo in interferometers:\n",
    "    print(\n",
    "        f\"Getting {ifo.name} analysis data segment (takes ~ 30s)\"\n",
    "    )\n",
    "    analysis_data = TimeSeries.fetch_open_data(\n",
    "        ifo.name, start_time, end_time\n",
    "    )\n",
    "    ifo.strain_data.roll_off = roll_off\n",
    "    ifo.strain_data.set_from_gwpy_timeseries(analysis_data)\n",
    "    raw_data[ifo.name] = analysis_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:56:01.989006Z",
     "start_time": "2023-02-14T14:56:01.366558Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot raw data:\n",
    "plot = GWpyPlot(figsize=(12, 4.8))\n",
    "ax = plot.add_subplot(xscale='auto-gps')\n",
    "for ifo_name, data in raw_data.items():\n",
    "    ax.plot(data, label=ifo_name)\n",
    "ax.set_epoch(1126259462.427)\n",
    "ax.set_xlim(1126259462, 1126259462.6)\n",
    "ax.set_ylabel('Strain noise')\n",
    "ax.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:03:08.158861Z",
     "start_time": "2023-02-14T14:03:08.153240Z"
    }
   },
   "source": [
    "Woah. That looks terrible. Where is the nobel-prize winning poster-child signal? \n",
    "\n",
    "We may need to clean up the data a bit to actually 'see' the signal. Lets get the data for the PSD and take a look at the noise once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T19:59:30.317237Z",
     "start_time": "2023-02-14T19:57:56.527482Z"
    },
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "# downloading data\n",
    "psd_start_time = start_time + duration\n",
    "psd_duration = 128\n",
    "psd_end_time = psd_start_time + psd_duration\n",
    "psd_tukey_alpha = 2 * roll_off / duration\n",
    "overlap = duration / 2\n",
    "\n",
    "for interferometer in interferometers:\n",
    "    print(\n",
    "        f\"Getting {interferometer.name} PSD data segment (takes ~ 1min)\"\n",
    "    )\n",
    "    psd_data = TimeSeries.fetch_open_data(\n",
    "        interferometer.name, psd_start_time, psd_end_time\n",
    "    )\n",
    "    psd = psd_data.psd(\n",
    "        fftlength=duration, overlap=overlap, window=(\"tukey\", psd_tukey_alpha),\n",
    "        method=\"median\"\n",
    "    )\n",
    "    interferometer.power_spectral_density = PowerSpectralDensity(\n",
    "        frequency_array=psd.frequencies.value, psd_array=psd.value\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T20:00:07.073739Z",
     "start_time": "2023-02-14T20:00:05.358931Z"
    }
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "for interferometer in interferometers:\n",
    "    analysis_data = abs(interferometer.frequency_domain_strain)\n",
    "    plt.loglog(interferometer.frequency_array, analysis_data, label=\"Analysis Data\")\n",
    "    plt.loglog(interferometer.frequency_array, abs(interferometer.amplitude_spectral_density_array),\n",
    "               label=\"ASD (estimated noise)\")\n",
    "    plt.xlim(interferometer.minimum_frequency, interferometer.maximum_frequency)\n",
    "    ymin_max = [min(analysis_data), max(analysis_data)]\n",
    "    plt.vlines([60, 120], *ymin_max, ls=\"--\", color='k', zorder=-10)\n",
    "    plt.fill_betweenx(ymin_max, 50, 250, color='tab:green', alpha=0.1)\n",
    "    plt.xlabel(\"Frequency [Hz]\")\n",
    "    plt.ylabel(r'Strain [strain/$\\sqrt{\\rm Hz}$]')\n",
    "    plt.title(f\"{interferometer.name} data\")\n",
    "    plt.ylim(*ymin_max)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets `notch` out the 60 and 120 Hz `violin` modes (black vertical lines), and only keep data within the 50-250Hz range (marked in green) from the raw data and re-plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T20:00:07.748463Z",
     "start_time": "2023-02-14T20:00:07.076274Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plot = GWpyPlot(figsize=(12, 4.8))\n",
    "ax = plot.add_subplot(xscale='auto-gps')\n",
    "for ifo_name, data in raw_data.items():\n",
    "    filtered_data = data.bandpass(50, 250).notch(60).notch(120)\n",
    "    ax.plot(filtered_data, label=ifo_name)\n",
    "ax.set_epoch(1126259462.427)\n",
    "ax.set_xlim(1126259462, 1126259462.6)\n",
    "ax.set_ylim(-1e-21, 1e-21)\n",
    "ax.set_ylabel('Strain noise')\n",
    "ax.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:10:38.215519Z",
     "start_time": "2023-02-14T14:10:38.210484Z"
    }
   },
   "source": [
    "Noiceee... Lets plot the signals in the frequency domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T20:00:11.757146Z",
     "start_time": "2023-02-14T20:00:07.750875Z"
    }
   },
   "outputs": [],
   "source": [
    "tc = trigger_time\n",
    "\n",
    "for ifo_name, data in raw_data.items():\n",
    "    qtrans = data.q_transform(\n",
    "        frange=(20,500), fres = 0.05,\n",
    "        outseg=(tc-0.2, tc+0.1)\n",
    "    )\n",
    "    plot = qtrans.plot(cmap = 'viridis', dpi = 150)\n",
    "    ax = plot.gca()\n",
    "    ax.set_title(f'{ifo_name} Q-transform')\n",
    "    ax.set_epoch(trigger_time)\n",
    "    ax.set_yscale('log')\n",
    "    ax.colorbar(label=\"Normalised energy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T14:12:25.228442Z",
     "start_time": "2023-02-14T14:12:25.224384Z"
    }
   },
   "source": [
    "## Getting priors\n",
    "\n",
    "Now lets write down our priors for this event's analysis. Note that again we set several delta functions and restrict the search space to speed up analysis for the sake of this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T20:00:11.786077Z",
     "start_time": "2023-02-14T20:00:11.760383Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup the prior\n",
    "from bilby.core.prior import Uniform, PowerLaw, Sine, Constraint, Cosine\n",
    "from corner import corner\n",
    "import pandas as pd\n",
    "\n",
    "# typically we would use a priors with wide bounds:\n",
    "tc = trigger_time\n",
    "priors = BBHPriorDict(dict(\n",
    "    mass_ratio=Uniform(name='mass_ratio', minimum=0.125, maximum=1),\n",
    "    chirp_mass=Uniform(name='chirp_mass', minimum=25, maximum=31),\n",
    "    mass_1=Constraint(name='mass_1', minimum=10, maximum=80),\n",
    "    mass_2=Constraint(name='mass_2', minimum=10, maximum=80),\n",
    "    a_1=Uniform(name='a_1', minimum=0, maximum=0.99),\n",
    "    a_2=Uniform(name='a_2', minimum=0, maximum=0.99),\n",
    "    tilt_1=Sine(name='tilt_1'),\n",
    "    tilt_2=Sine(name='tilt_2'),\n",
    "    phi_12=Uniform(name='phi_12', minimum=0, maximum=2 * np.pi, boundary='periodic'),\n",
    "    phi_jl=Uniform(name='phi_jl', minimum=0, maximum=2 * np.pi, boundary='periodic'),\n",
    "    luminosity_distance=PowerLaw(alpha=2, name='luminosity_distance', minimum=50, maximum=2000),\n",
    "    dec=Cosine(name='dec'),\n",
    "    ra=Uniform(name='ra', minimum=0, maximum=2 * np.pi, boundary='periodic'),\n",
    "    theta_jn=Sine(name='theta_jn'),\n",
    "    psi=Uniform(name='psi', minimum=0, maximum=np.pi, boundary='periodic'),\n",
    "    phase=Uniform(name='phase', minimum=0, maximum=2 * np.pi, boundary='periodic'),\n",
    "    geocent_time=Uniform(minimum=tc - 0.1, maximum=tc + 0.1, latex_label=\"$t_c$\", unit=\"$s$\")\n",
    "))\n",
    "\n",
    "# HACK: for this example (to make analysis faster) we will use a prior with tighter bounds\n",
    "priors['luminosity_distance'] = 419.18\n",
    "priors['mass_1'] = Constraint(name='mass_1', minimum=30, maximum=50)\n",
    "priors['mass_2'] = Constraint(name='mass_2', minimum=20, maximum=40)\n",
    "priors['ra'] = 2.269\n",
    "priors['dec'] = -1.223\n",
    "priors['geocent_time'] = tc\n",
    "priors['theta_jn'] = 2.921\n",
    "priors['phi_jl'] = 0.968\n",
    "priors['psi'] = 2.659\n",
    "# dont do this in a real run \n",
    "\n",
    "prior_samples = priors.sample(10000)\n",
    "prior_samples_df = pd.DataFrame(prior_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots of some priors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T20:00:12.888429Z",
     "start_time": "2023-02-14T20:00:11.788116Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "parameters = ['chirp_mass', 'mass_ratio', 'a_1', 'a_2']\n",
    "fig = corner(\n",
    "    prior_samples_df[parameters], plot_datapoints=False, \n",
    "    plot_contours=False, plot_density=True,\n",
    "    color=\"tab:gray\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert some parameters and see what the prior-distributions we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T20:00:13.391686Z",
     "start_time": "2023-02-14T20:00:12.891233Z"
    }
   },
   "outputs": [],
   "source": [
    "from bilby.gw.conversion import generate_mass_parameters\n",
    "\n",
    "prior_samples = generate_mass_parameters(prior_samples)\n",
    "prior_samples['cos_tilt_1'] = np.cos(prior_samples['tilt_1'])\n",
    "prior_samples['cos_tilt_2'] = np.cos(prior_samples['tilt_2'])\n",
    "s1z = prior_samples[\"a_1\"] * prior_samples['cos_tilt_1']\n",
    "s2z = prior_samples[\"a_2\"] * prior_samples['cos_tilt_2']\n",
    "q = prior_samples['mass_ratio']\n",
    "prior_samples['chi_eff'] = (s1z + s2z * q) / (1 + q)\n",
    "prior_samples_df = pd.DataFrame(prior_samples)\n",
    "prior_samples_df\n",
    "\n",
    "parameters = ['mass_1', 'mass_2', 'chi_eff']\n",
    "fig = corner(prior_samples_df[parameters], plot_datapoints=False, plot_contours=False, plot_density=True,\n",
    "             color=\"tab:gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T20:00:19.501482Z",
     "start_time": "2023-02-14T20:00:13.393898Z"
    }
   },
   "outputs": [],
   "source": [
    "waveform_generator = WaveformGenerator(\n",
    "    duration=interferometers.duration,\n",
    "    sampling_frequency=interferometers.sampling_frequency,\n",
    "    frequency_domain_source_model=lal_binary_black_hole,\n",
    "    parameter_conversion=convert_to_lal_binary_black_hole_parameters,\n",
    "    waveform_arguments=dict(\n",
    "        waveform_approximant=\"IMRPhenomPv2\",\n",
    "        reference_frequency=20)\n",
    ")\n",
    "\n",
    "likelihood = GravitationalWaveTransient(\n",
    "    interferometers=interferometers, waveform_generator=waveform_generator,\n",
    "    priors=priors, time_marginalization=False, distance_marginalization=False,\n",
    "    phase_marginalization=True, jitter_time=False\n",
    ")\n",
    "\n",
    "\n",
    "if RE_RUN_SLOW_CELLS:\n",
    "    bilby_logger.setLevel(logging.INFO)\n",
    "    result = run_sampler(\n",
    "        likelihood=likelihood, priors=priors, save=True,\n",
    "        label=\"GW150914\",\n",
    "        nlive=50, walks=25, # HACK: use defaults nlive/walks (much more aggressive)\n",
    "        conversion_function=generate_all_bbh_parameters,\n",
    "        result_class=CBCResult,\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping sampling...\")\n",
    "    fn = f\"{OUTDIR}/GW150914_result.json\"\n",
    "    download(GW150914_URL, fn)\n",
    "    result = bilby.gw.result.CBCResult.from_json(filename=fn)\n",
    "    print(\"Loaded result!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T22:32:03.909979Z",
     "start_time": "2023-02-14T22:32:03.369538Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = result.plot_corner(parameters=[\"mass_ratio\", \"chi_eff\"], save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T22:31:45.735944Z",
     "start_time": "2023-02-14T22:31:43.376653Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = result.plot_corner(parameters=[\"mass_1\", \"mass_2\"], save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: To get some proper results, we'd have to run with more robust sampler settings and wider priors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing GWTC parameter estimation results \n",
    "\n",
    "The GWTC data and parameter estimation results are all online (eg https://zenodo.org/record/5546663).\n",
    "Lets download one result and make some plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T21:23:05.922070Z",
     "start_time": "2023-02-14T21:22:32.908645Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "OUTDIR=\"outdir/\"\n",
    "GW200308_fn = f\"{OUTDIR}/GW200308_result.h5\"\n",
    "download(GW200308_URL, GW200308_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T21:25:58.548980Z",
     "start_time": "2023-02-14T21:25:58.542660Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "GW200308_result = h5py.File(GW200308_fn, \"r\")\n",
    "print(\"GW200308 samples loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T21:27:23.785081Z",
     "start_time": "2023-02-14T21:27:23.780752Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(GW200308_result.keys()))\n",
    "print(list(GW200308_result['C01:IMRPhenomXPHM'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T21:44:04.419828Z",
     "start_time": "2023-02-14T21:44:04.412471Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Parameters stored:\")\n",
    "for i in GW200308_result['C01:IMRPhenomXPHM']['posterior_samples'].dtype.names:\n",
    "    print(f\"  {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T21:49:18.961193Z",
     "start_time": "2023-02-14T21:49:18.750091Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "xphm_mc = GW200308_result['C01:IMRPhenomXPHM']['posterior_samples']['chirp_mass'][:]\n",
    "seob_mc = GW200308_result['C01:SEOBNRv4PHM']['posterior_samples']['chirp_mass'][:]\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "bins = np.linspace(0, 250, 50)\n",
    "ax.hist(xphm_mc, bins=bins, density=True, histtype='stepfilled', label='XPHM',  lw=2,alpha=0.25)\n",
    "ax.hist(seob_mc, bins=bins,  density=True, histtype='stepfilled', label='SEOB', lw=2,alpha=0.25)\n",
    "ax.set_yticks([])\n",
    "ax.legend(frameon=False)\n",
    "ax.set_xlabel(\"Chirp-mass Msun\");"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
